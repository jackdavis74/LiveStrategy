{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5535c3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import requests\n",
    "import numpy as np\n",
    "from polygon.rest import RESTClient\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import pytz\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import concurrent.futures\n",
    "import json\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "api_key = 'hFrBS7nzcaLTa8mplO1ejm44DI4EscDM'\n",
    "client = RESTClient(api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "808d8276",
   "metadata": {},
   "outputs": [],
   "source": [
    "def est_all(unix_ms_timestamp):\n",
    "    # Convert Unix timestamp in milliseconds to seconds\n",
    "    unix_seconds = unix_ms_timestamp / 1000.0\n",
    "    # Create a datetime object from the Unix timestamp\n",
    "    utc_time = datetime.utcfromtimestamp(unix_seconds)\n",
    "    # Define the UTC and EST timezones\n",
    "    utc_zone = pytz.utc\n",
    "    est_zone = pytz.timezone('US/Eastern')\n",
    "    # Localize the UTC datetime object to UTC timezone\n",
    "    utc_time = utc_zone.localize(utc_time)\n",
    "    # Convert the UTC time to EST\n",
    "    est_time = utc_time.astimezone(est_zone)\n",
    "    est_time = est_time.replace(tzinfo=None)\n",
    "    return est_time.strftime('%Y-%m-%d %H:%M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20d9d155",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Minute aggregate-based variable function\n",
    "def minute_data(ticker, min_date, max_date):\n",
    "    try:\n",
    "        # Base URL for the initial request\n",
    "        base_url = f'https://api.polygon.io/v2/aggs/ticker/{ticker}/range/1/minute/{min_date}/{max_date}?adjusted=false&sort=asc&limit=50000&apiKey={api_key}'\n",
    "        all_data = []\n",
    "        url = base_url\n",
    "        while url:\n",
    "            response = requests.get(url)\n",
    "            data = response.json()\n",
    "            if not 'results' in data:\n",
    "                return pd.DataFrame()\n",
    "            all_data.extend(data['results'])\n",
    "            next_url = data.get('next_url', None)\n",
    "            if next_url:\n",
    "                url = f\"{next_url}&apiKey={api_key}\"\n",
    "            else:\n",
    "                break\n",
    "        df = pd.DataFrame(all_data)\n",
    "        #Get dates/times, filter\n",
    "        df['date_time'] = df['t'].apply(est_all)\n",
    "        df[['date', 'time']] = df['date_time'].str.split(' ', expand=True)\n",
    "        df = df[(df['date'] >= min_date) & (df['date'] < max_date)]\n",
    "        df = df[(df['time'] >= '09:30') & (df['time'] <= '15:59')]\n",
    "        #Create key\n",
    "        df['key'] = ticker + '_' + df['date']\n",
    "        #create vwap df\n",
    "        times_all = [f'{h:02}:{m:02}' for h in range(9, 16) for m in range(30 if h == 9 else 0, 60)]\n",
    "        df_vwap = df.pivot(index='key', columns='time', values='vw')\n",
    "        df_vwap = df_vwap.reindex(columns=times_all)\n",
    "        df_vwap = df_vwap.reset_index()\n",
    "        df_vwap.columns = [f'{col}_vw' if col != 'key' else col for col in df_vwap.columns]\n",
    "        #create volume df\n",
    "        df_vol = df.pivot(index='key', columns='time', values='v')\n",
    "        df_vol = df_vol.reindex(columns=times_all)\n",
    "        df_vol = df_vol.reset_index()\n",
    "        df_vol.columns = [f'{col}_vol' if col != 'key' else col for col in df_vol.columns]\n",
    "        #get 15:58,15:59 close columns\n",
    "        df_c = df.pivot(index='key', columns='time', values='c')\n",
    "        df_c = df_c.reindex(columns=['15:58','15:59'])\n",
    "        df_c = df_c.reset_index()\n",
    "        df_c.columns = [f'{col}_c' if col != 'key' else col for col in df_c.columns]\n",
    "        #get 15:59 open column\n",
    "        df_o = df.pivot(index='key', columns='time', values='o')\n",
    "        df_o = df_o.reindex(columns=['15:59'])\n",
    "        df_o = df_o.reset_index()\n",
    "        df_o.columns = [f'{col}_o' if col != 'key' else col for col in df_o.columns]\n",
    "        #merge all\n",
    "        mer = df_vwap.merge(df_vol, on='key', how='left')\n",
    "        mer = mer.merge(df_c, on='key',how='left')\n",
    "        mer = mer.merge(df_o, on='key',how='left')\n",
    "        return mer\n",
    "    except Exception as e:\n",
    "        print(f'{ticker}:{e}')\n",
    "        return pd.DataFrame()\n",
    "#minute_data('AAPL','2024-01-01','2024-03-01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2105190",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 21/182 [28:03<5:05:52, 113.99s/it] IOStream.flush timed out\n",
      "100%|██████████| 182/182 [54:13<00:00, 17.88s/it]  \n",
      "100%|██████████| 182/182 [27:17<00:00,  9.00s/it] \n",
      "100%|██████████| 182/182 [32:33<00:00, 10.73s/it]  \n",
      "100%|██████████| 182/182 [26:27<00:00,  8.72s/it]  \n",
      "100%|██████████| 181/181 [30:59<00:00, 10.27s/it] \n",
      "100%|██████████| 181/181 [28:44<00:00,  9.53s/it] \n",
      "100%|██████████| 181/181 [19:06<00:00,  6.34s/it] \n",
      "100%|██████████| 181/181 [09:48<00:00,  3.25s/it]\n",
      "100%|██████████| 181/181 [07:04<00:00,  2.34s/it]\n"
     ]
    }
   ],
   "source": [
    "for x in range(0,9):\n",
    "    feed = pd.read_csv(f'feed{x}.csv')\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        results = list(tqdm(executor.map(minute_data, feed['ticker'],feed['min_date'],feed['max_date']), total=len(feed)))\n",
    "    min_df = pd.concat(results, ignore_index=True)\n",
    "    min_df.to_csv(f'out{x}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "652cddfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 181/181 [32:59<00:00, 10.93s/it]  \n"
     ]
    }
   ],
   "source": [
    "x = 9\n",
    "feed = pd.read_csv(f'feed{x}.csv')\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    results = list(tqdm(executor.map(minute_data, feed['ticker'],feed['min_date'],feed['max_date']), total=len(feed)))\n",
    "min_df = pd.concat(results, ignore_index=True)\n",
    "min_df.to_csv(f'out{x}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00615c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
